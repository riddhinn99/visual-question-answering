{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras==2.1.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (2.1.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from keras==2.1.0) (5.1.1)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from keras==2.1.0) (1.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from keras==2.1.0) (1.16.4)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from keras==2.1.0) (1.3.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "import os, argparse\n",
    "import cv2, spacy, numpy as np\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.externals import joblib\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "K.set_image_data_format('channels_first')\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0324 15:50:24.692838 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:63: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0324 15:50:24.703495 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:492: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0324 15:50:24.873875 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3630: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0324 15:50:24.906625 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:158: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0324 15:50:24.908394 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:163: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0324 15:50:24.909402 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0324 15:50:24.998822 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:172: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "W0324 15:50:25.000154 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "W0324 15:50:25.111370 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:188: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "W0324 15:50:25.196090 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3458: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0324 15:50:25.402793 140735559299968 deprecation.py:506] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model = vgg16.VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "def get_image_features_without_fd(image_file_name,model):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    img = image.load_img(image_file_name, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    model_extractfeatures = Model(input=model.input, output=model.get_layer('fc2').output)\n",
    "    fc2_features = model_extractfeatures.predict(x)\n",
    "    image_features = fc2_features.reshape(4096)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_model_without():\n",
    "    ''' Takes the CNN weights file, and returns the VGG model update \n",
    "    with the weights. Requires the file VGG.py inside models/CNN '''\n",
    "    image_model = VGG16(weights='imagenet', include_top=False)\n",
    "    image_model.layers.pop()\n",
    "    image_model.layers.pop()\n",
    "    # this is standard VGG 16 without the last two layers\n",
    "    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    # one may experiment with \"adam\" optimizer, but the loss function for\n",
    "    # this kind of task is pretty standard\n",
    "    image_model.compile(optimizer=sgd, loss='categorical_crossentropy')\n",
    "    return image_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0324 15:51:09.998320 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:711: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0324 15:51:10.004201 140735559299968 deprecation.py:506] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "W0324 15:51:10.015716 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2884: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vgg_without = get_image_model_without()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_question_features_without_fd(question):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    word_embeddings = spacy.load('en_core_web_md')\n",
    "    tokens = word_embeddings(question)\n",
    "    question_tensor = np.zeros((30, 300))\n",
    "    for j in range(len(tokens)):\n",
    "        question_tensor[j,:] = tokens[j].vector\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "model = vgg16.VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_image_features_without_fd(image_file_name,model):\n",
    "    ''' Runs the given image_file to VGG 16 model and returns the \n",
    "    weights (filters) as a 1, 4096 dimension vector '''\n",
    "    img = image.load_img(image_file_name, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    model_extractfeatures = Model(input=model.input, output=model.get_layer('fc2').output)\n",
    "    fc2_features = model_extractfeatures.predict(x)\n",
    "    image_features = fc2_features.reshape(4096)\n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VQA_model():\n",
    "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
    "\n",
    "    # thanks the keras function for loading a model from JSON, this becomes\n",
    "    # very easy to understand and work. Alternative would be to load model\n",
    "    # from binary like cPickle but then model would be obfuscated to users\n",
    "    with open('our_model_final.json','r') as f:\n",
    "      vqa_model = keras.models.model_from_json(f.read())\n",
    "    # vqa_model.load_weights(VQA_weights_file_name)\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0324 15:59:43.801305 140735559299968 deprecation.py:506] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3013: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_2 (Merge)              (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              4719616   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22)                22550     \n",
      "=================================================================\n",
      "Total params: 12,704,790\n",
      "Trainable params: 12,704,790\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "our_vqa_model = get_VQA_model()\n",
    "our_vqa_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_md==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz#egg=en_core_web_md==2.2.5 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: thinc==7.4.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (41.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.16.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (1.5.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/riddhi_11/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (0.5.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_question_features_without_fd(question,word_embeddings):\n",
    "    ''' For a given question, a unicode string, returns the time series vector\n",
    "    with each word (token) transformed into a 300 dimension representation\n",
    "    calculated using Glove Vector '''\n",
    "    tokens = word_embeddings(question)\n",
    "    question_tensor = np.zeros((30, 300))\n",
    "    for j in range(len(tokens)):\n",
    "        question_tensor[j,:] = tokens[j].vector\n",
    "    return question_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_feature_test(image_path,model):\n",
    "  image_features = get_image_features_without_fd(image_path,model)\n",
    "  image_feature_processed = np.array(image_features)\n",
    "  image_feature_processed = image_feature_processed.reshape((1,image_feature_processed.shape[0]))\n",
    "  return image_feature_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = create_image_feature_test('sample_data_new/7.jpg',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image2 = create_image_feature_test('sample_data_new/car2.jpg',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image3 = create_image_feature_test('car/2.jpg',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_question_feature_test(text):\n",
    "  question_feature=get_question_features_without_fd(text,word_embeddings)\n",
    "  question_feature_processed = np.array(question_feature)  \n",
    "  question_feature_processed = question_feature_processed.reshape((1,question_feature_processed.shape[0], question_feature_processed.shape[1]))\n",
    "  return question_feature_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Imageid', 'Question', 'Answer']\n",
      "['1', 'is this plane', 'yes']\n",
      "['1', 'is plane flying', 'no']\n",
      "['2', 'is this plane', 'yes']\n",
      "['2', 'is plane flying', 'yes']\n",
      "['3', 'is this jeep or car', 'jeep']\n",
      "['4', 'is this jeep ', 'yes']\n",
      "['4', 'is jeep yellow or red', 'yellow']\n",
      "['4', 'is it front side of jeep', 'yes']\n",
      "['5', 'is jeep black', 'yes']\n",
      "['6', 'what color is car', 'green']\n",
      "['6', 'is it front sideof car', 'no']\n",
      "['7', 'what color is car', 'red']\n",
      "['7', 'is this car or jeep', 'car']\n",
      "['8', 'what is color of car', 'white']\n",
      "['9', 'is car white', 'yes']\n",
      "['10', 'is bike or car', 'bike']\n",
      "['11', 'is car black', 'yes']\n",
      "['12', 'is this back side of car', 'yes']\n",
      "['13', 'how many cats in this picture', '2']\n",
      "['14', 'is this cat', 'yes']\n",
      "['15', 'is cat sleeping', 'yes']\n",
      "['16', 'is cat sleeping', 'yes']\n",
      "['17', 'what is color of cat', 'black']\n",
      "['18', 'is cat black', 'no']\n",
      "['19', 'is this cat or dog', 'dog']\n",
      "['20', 'is dog black or white', 'black']\n",
      "['21', 'is dog white or black', 'white']\n",
      "['22', 'is this dog', 'yes']\n",
      "['23', 'is dog black or white', 'black']\n",
      "['24', 'is dog fluffy', 'yes']\n",
      "['25', 'is this flower', 'yes']\n",
      "['26', 'is this rose or lotus', 'rose']\n",
      "['27', 'what is flower', 'hibiscus']\n",
      "['28', 'is this flower', 'no']\n",
      "['29', 'is there single flower', 'no']\n",
      "['30', 'is this fruit or vegetable', 'fruit']\n",
      "['31', 'what fruit is this', 'pear']\n",
      "['32', 'what fruit is this', 'lemon']\n",
      "['33', 'is fruit apple', 'yes']\n",
      "['34', 'what fruit is this', 'orange']\n",
      "['35', 'is this apple', 'yes']\n",
      "['36', 'is this orange', 'no']\n",
      "['37', 'what fruit is this', 'apple']\n",
      "['38', 'what fruit is this', 'orange']\n",
      "['39', 'is this banana', 'no']\n",
      "['40', 'is this pear', 'yes']\n",
      "['41', 'is this pineapple', 'yes']\n",
      "['42', 'what is the body part in xray', 'leg']\n",
      "['42', 'is this banana', 'yes']\n",
      "['42', 'what fruit is this', 'banana']\n",
      "['43', 'is this apple', 'no']\n",
      "['44', 'is this strawberry', 'no']\n",
      "['45', 'is this strawberry', 'yes']\n",
      "['46', 'is this person', 'yes']\n",
      "['47', 'is this person male or female', 'female']\n",
      "['48', 'is female smiling', 'yes']\n",
      "['49', 'is this female', 'yes']\n",
      "['50', 'is male smiling', 'yes']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_datas = []\n",
    "with open('sample_data_new/training.csv') as csv_file:\n",
    "  csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "  for row in csv_reader:\n",
    "    print (row)\n",
    "    training_datas.append(row)\n",
    "training_datas = training_datas[1:]\n",
    "len(training_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "['yes', 'no', 'yes', 'yes', 'jeep', 'yes', 'yellow', 'yes', 'yes', 'green', 'no', 'red', 'car', 'white', 'yes', 'bike', 'yes', 'yes', '2', 'yes', 'yes', 'yes', 'black', 'no', 'dog', 'black', 'white', 'yes', 'black', 'yes', 'yes', 'rose', 'hibiscus', 'no', 'no', 'fruit', 'pear', 'lemon', 'yes', 'orange', 'yes', 'no', 'apple', 'orange', 'no', 'yes', 'yes', 'leg', 'yes', 'banana', 'no', 'no', 'yes', 'yes', 'female', 'yes', 'yes', 'yes']\n"
     ]
    }
   ],
   "source": [
    "trainY= []\n",
    "for training_data in training_datas:\n",
    "  img_id,text,output = training_data\n",
    "  trainY.append(output)\n",
    "print(len(trainY))\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 5, 5, 6, 5, 19, 5, 5, 18, 14, 0, 21, 4, 5, 1, 5, 5, 15, 5, 5, 5, 7, 14, 3, 7, 4, 5, 7, 5, 5, 13, 9, 14, 14, 20, 10, 17, 5, 12, 5, 14, 2, 12, 14, 5, 5, 11, 5, 16, 14, 14, 5, 5, 8, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "set_trainy = list(set(trainY))\n",
    "trainY_labels = []\n",
    "for y in trainY:\n",
    "  trainY_labels.append(set_trainy.index(y))\n",
    "print(trainY_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_cate = keras.utils.to_categorical(trainY_labels, num_classes=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 image\n",
      "Processing 1 image\n",
      "Processing 2 image\n",
      "Processing 2 image\n",
      "Processing 3 image\n",
      "Processing 4 image\n",
      "Processing 4 image\n",
      "Processing 4 image\n",
      "Processing 5 image\n",
      "Processing 6 image\n",
      "Processing 6 image\n",
      "Processing 7 image\n",
      "Processing 7 image\n",
      "Processing 8 image\n",
      "Processing 9 image\n",
      "Processing 10 image\n",
      "Processing 11 image\n",
      "Processing 12 image\n",
      "Processing 13 image\n",
      "Processing 14 image\n",
      "Processing 15 image\n",
      "Processing 16 image\n",
      "Processing 17 image\n",
      "Processing 18 image\n",
      "Processing 19 image\n",
      "Processing 20 image\n",
      "Processing 21 image\n",
      "Processing 22 image\n",
      "Processing 23 image\n",
      "Processing 24 image\n",
      "Processing 25 image\n",
      "Processing 26 image\n",
      "Processing 27 image\n",
      "Processing 28 image\n",
      "Processing 29 image\n",
      "Processing 30 image\n",
      "Processing 31 image\n",
      "Processing 32 image\n",
      "Processing 33 image\n",
      "Processing 34 image\n",
      "Processing 35 image\n",
      "Processing 36 image\n",
      "Processing 37 image\n",
      "Processing 38 image\n",
      "Processing 39 image\n",
      "Processing 40 image\n",
      "Processing 41 image\n",
      "Processing 42 image\n",
      "Processing 42 image\n",
      "Processing 42 image\n",
      "Processing 43 image\n",
      "Processing 44 image\n",
      "Processing 45 image\n",
      "Processing 46 image\n",
      "Processing 47 image\n",
      "Processing 48 image\n",
      "Processing 49 image\n",
      "Processing 50 image\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(58, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX= []\n",
    "for training_data in training_datas:\n",
    "  img_id,text,output = training_data\n",
    "  print(\"Processing {} image\".format(img_id))\n",
    "  image_features = get_image_features_without_fd(\"sample_data_new/{}.jpg\".format(img_id),model)\n",
    "  question_features = get_question_features_without_fd(text,word_embeddings)\n",
    "  trainX.append([question_features,image_features])\n",
    "np.array(trainX).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features_array = []\n",
    "image_features_array = []\n",
    "for x in trainX:\n",
    "  question_features_array.append(x[0])\n",
    "  image_features_array.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 30, 300)\n",
      "(58, 4096)\n"
     ]
    }
   ],
   "source": [
    "question_features_array = np.array(question_features_array)\n",
    "image_features_array = np.array(image_features_array)\n",
    "print(question_features_array.shape)\n",
    "print(image_features_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n",
      "['yes', 'no', 'yes', 'yes', 'jeep', 'yes', 'yellow', 'yes', 'yes', 'green', 'no', 'red', 'car', 'white', 'yes', 'bike', 'yes', 'yes', '2', 'yes', 'yes', 'yes', 'black', 'no', 'dog', 'black', 'white', 'yes', 'black', 'yes', 'yes', 'rose', 'hibiscus', 'no', 'no', 'fruit', 'pear', 'lemon', 'yes', 'orange', 'yes', 'no', 'apple', 'orange', 'no', 'yes', 'yes', 'leg', 'yes', 'banana', 'no', 'no', 'yes', 'yes', 'female', 'yes', 'yes', 'yes']\n"
     ]
    }
   ],
   "source": [
    "trainY= []\n",
    "for training_data in training_datas:\n",
    "  img_id,text,output = training_data\n",
    "  trainY.append(output)\n",
    "print(len(trainY))\n",
    "print(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 14, 5, 5, 6, 5, 19, 5, 5, 18, 14, 0, 21, 4, 5, 1, 5, 5, 15, 5, 5, 5, 7, 14, 3, 7, 4, 5, 7, 5, 5, 13, 9, 14, 14, 20, 10, 17, 5, 12, 5, 14, 2, 12, 14, 5, 5, 11, 5, 16, 14, 14, 5, 5, 8, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "set_trainy = list(set(trainY))\n",
    "trainY_labels = []\n",
    "for y in trainY:\n",
    "  trainY_labels.append(set_trainy.index(y))\n",
    "print(trainY_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_cate = keras.utils.to_categorical(trainY_labels, num_classes=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0324 16:02:38.964423 140735559299968 deprecation.py:323] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0324 16:02:40.879973 140735559299968 deprecation.py:506] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:675: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0324 16:02:40.945062 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:953: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0324 16:02:40.953615 140735559299968 module_wrapper.py:139] From /Users/riddhi_11/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:940: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "58/58 [==============================] - 18s 314ms/step - loss: 4.4611\n",
      "Epoch 2/30\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 3.7696\n",
      "Epoch 3/30\n",
      "58/58 [==============================] - 12s 206ms/step - loss: 3.7906\n",
      "Epoch 4/30\n",
      "58/58 [==============================] - 11s 189ms/step - loss: 2.8739\n",
      "Epoch 5/30\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 2.2513\n",
      "Epoch 6/30\n",
      "58/58 [==============================] - 11s 185ms/step - loss: 2.3616\n",
      "Epoch 7/30\n",
      "58/58 [==============================] - 12s 206ms/step - loss: 2.5490\n",
      "Epoch 8/30\n",
      "58/58 [==============================] - 11s 192ms/step - loss: 1.9208\n",
      "Epoch 9/30\n",
      "58/58 [==============================] - 12s 206ms/step - loss: 1.9129\n",
      "Epoch 10/30\n",
      "58/58 [==============================] - 11s 191ms/step - loss: 1.7532\n",
      "Epoch 11/30\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 1.6857\n",
      "Epoch 12/30\n",
      "58/58 [==============================] - 11s 183ms/step - loss: 1.7113\n",
      "Epoch 13/30\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 1.3280\n",
      "Epoch 14/30\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 1.3375\n",
      "Epoch 15/30\n",
      "58/58 [==============================] - 12s 210ms/step - loss: 1.3389\n",
      "Epoch 16/30\n",
      "58/58 [==============================] - 11s 192ms/step - loss: 1.5020\n",
      "Epoch 17/30\n",
      "58/58 [==============================] - 11s 193ms/step - loss: 1.2551\n",
      "Epoch 18/30\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 1.1535\n",
      "Epoch 19/30\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.8453\n",
      "Epoch 20/30\n",
      "58/58 [==============================] - 12s 201ms/step - loss: 0.8451\n",
      "Epoch 21/30\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.9315\n",
      "Epoch 22/30\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.9817\n",
      "Epoch 23/30\n",
      "58/58 [==============================] - 11s 184ms/step - loss: 1.0451\n",
      "Epoch 24/30\n",
      "58/58 [==============================] - 12s 199ms/step - loss: 0.9514\n",
      "Epoch 25/30\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.8120\n",
      "Epoch 26/30\n",
      "58/58 [==============================] - 12s 209ms/step - loss: 0.9331\n",
      "Epoch 27/30\n",
      "58/58 [==============================] - 10s 177ms/step - loss: 1.0010\n",
      "Epoch 28/30\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.6060\n",
      "Epoch 29/30\n",
      "58/58 [==============================] - 11s 186ms/step - loss: 0.7600\n",
      "Epoch 30/30\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5975\n"
     ]
    }
   ],
   "source": [
    "history = our_vqa_model.fit([question_features_array,image_features_array],trainY_cate,5,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_weights_after_training_jupyter_test.h5'\n",
    "outfile = open(filename,'wb')\n",
    "our_vqa_model.save_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model architecture\n",
    "model_arch = 'model_architecture_after_training_jupyter_test.json'\n",
    "outfile = open(model_arch,'wb')\n",
    "with open(model_arch, 'w') as f:\n",
    "    f.write(our_vqa_model.to_json())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD the model architecture\n",
    "model_arch_read = 'model_architecture_after_training_jupyter_test.json'\n",
    "\n",
    "with open(model_arch_read, 'r') as f:\n",
    "    vqa_model = model_from_json(f.read())\n",
    "\n",
    "# Load weights into the new model\n",
    "vqa_model.load_weights(filename);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_question = create_question_feature_test(\"what is this\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image = create_image_feature_test('sample_data_new/test/37.jpg',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "print(vqa_model.predict_classes([new_question,new_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 67ms/step\n",
      "['red', 'bike', 'apple', 'dog', 'white', 'yes', 'jeep', 'black', 'female', 'hibiscus', 'pear', 'leg', 'orange', 'rose', 'no', '2', 'banana', 'lemon', 'green', 'yellow', 'fruit', 'car']\n"
     ]
    }
   ],
   "source": [
    "result=set_trainy[vqa_model.predict_classes([new_question,new_image])[0]]\n",
    "print(set_trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
